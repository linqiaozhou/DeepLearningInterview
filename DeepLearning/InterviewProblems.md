### 腾讯优图视觉算法岗
#### 1. 介绍LSTM和GRU的动机，原理和方法
答： 两者都是为了解决 RNN 中梯度消失严重，不能捕捉长程依赖的问题。注意讲清楚LSTM和GRU是为什么可以解决梯度消失的

#### 2. Batch Norm是什么？为什么需要它？以及Batch Norm是如何实现的？
答： 解决深度网络中Internal Covariate Shift，数据分布的变化，有利于加快网络收敛。实现方式：基于batch的统计采样，两个变量：average_mean, average std

#### 3. 卷积的循环实现方式？介绍caffe中im2col的实现方式。
答： 循环，滑动窗口； 待补充

#### 4. 1x1卷积的作用
答： 通道融合; 配合非线性激活，可以以较少的参数，增加网络的非线性; 降维/升维(主要是减少/增加feature map的channel数量)

#### 5. 对resnet/残差的理解
答： 直接拟合目标的难度要大于拟合残差和恒等映射的难度; shortcut 一定程度上解决了深度网络中梯度无法有效回传到浅层的问题

#### 6. 介绍下FPN
答： 待补充